{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "import timm\n",
    "from timm.models.helpers import model_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchprofile import profile_macs\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "from torch_scatter import scatter_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [model for model in timm.list_models('vit*')]\n",
    "# print([x for x in models if 'vitamin' not in str(x)])\n",
    "model_name = \"vit_base_patch16_224\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    def __init__(self, model_name, pretrained=True, pretrained_cfg=None, pretrained_cfg_overlay=None,\n",
    "                checkpoint_path='', scriptable=None, exportable=None, no_jit=True):\n",
    "        self.model_name = model_name\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, pretrained_cfg=pretrained_cfg, pretrained_cfg_overlay=pretrained_cfg_overlay,\n",
    "            checkpoint_path=checkpoint_path, scriptable=scriptable, exportable=exportable, no_jit=no_jit\n",
    "        )\n",
    "        self.profiler = None  \n",
    "        \n",
    "        # print(self.model.default_cfg)\n",
    "        # params = sum(p.numel() for p in self.model.parameters())\n",
    "        # print(f\"Number of parameters: {params / 1e6:.2f}M\")\n",
    "    \n",
    "    def inference_one(self, input_tensor: torch.Tensor):\n",
    "        \"\"\"Run inference on a single input tensor.\"\"\"\n",
    "        self.model.eval()  \n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "        return output\n",
    "    \n",
    "    def start_profiler(self) -> None:\n",
    "        \"\"\"Initialize and start the profiler.\"\"\"\n",
    "        self.profiler = profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n",
    "            record_shapes=True,\n",
    "            with_stack=True,\n",
    "            with_flops=True,\n",
    "            profile_memory=True,\n",
    "            with_modules=True\n",
    "        )\n",
    "        self.profiler.__enter__()  # Start the profiler context manually\n",
    "\n",
    "    def stop_profiler(self) -> None:\n",
    "        \"\"\"Stop the profiler and process the collected data.\"\"\"\n",
    "        if self.profiler:\n",
    "            self.profiler.__exit__(None, None, None) \n",
    "\n",
    "    def list_events(self) -> None:\n",
    "        \"\"\"\n",
    "        List all recorded events from the profiler.\t\n",
    "        \n",
    "        name\t         - The name of the operation (e.g., aten::add, aten::matmul, aten::conv2d).\n",
    "\t\tcpu_time_total\t - Total time spent on the CPU for this operation, in microseconds.\n",
    "\t\tcuda_time_total\t - Total time spent on the GPU for this operation, in microseconds.\n",
    "\t\tinput_shapes\t - Shapes of the tensors used as inputs to this operation.\n",
    "\t\toutput_shapes\t - Shapes of the tensors produced by this operation (if applicable).\n",
    "\t\tdevice_type\t     - Whether the operation was executed on CPU or CUDA.\n",
    "\t\tdevice\t         - The device ID on which the operation was executed.\n",
    "\t\tself_cpu_time\t - Time spent on the CPU for this operation alone (excluding time for child operations).\n",
    "\t\tself_cuda_time   - Time spent on the GPU for this operation alone (excluding time for child operations).\n",
    "\t\t\"\"\"\n",
    "        if self.profiler is None:\n",
    "            print(\"Profiler has not been initialized or profiling session has ended.\")\n",
    "        else:\n",
    "            print(self.profiler.key_averages()) \n",
    "            for event in self.profiler.events():\n",
    "                print(f\"Name: {event.name}, CPU Time: {event.cpu_time_total}, CUDA Time: {event.cuda_time_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.profiler' has no attribute 'export_chrome_trace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m Analyzer(model_name)\n\u001b[0;32m      2\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_profiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m analyzer\u001b[38;5;241m.\u001b[39minference_one(input_tensor)\n\u001b[0;32m      5\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mstop_profiler()\n",
      "Cell \u001b[1;32mIn[65], line 26\u001b[0m, in \u001b[0;36mAnalyzer.start_profiler\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_profiler\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize and start the profiler.\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;241m=\u001b[39m profile(\n\u001b[0;32m     25\u001b[0m         activities\u001b[38;5;241m=\u001b[39m[ProfilerActivity\u001b[38;5;241m.\u001b[39mCPU, ProfilerActivity\u001b[38;5;241m.\u001b[39mCUDA],\n\u001b[1;32m---> 26\u001b[0m         on_trace_ready\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_chrome_trace\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     27\u001b[0m         record_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     28\u001b[0m         with_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     29\u001b[0m         with_flops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m         profile_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m         with_modules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.profiler' has no attribute 'export_chrome_trace'"
     ]
    }
   ],
   "source": [
    "analyzer = Analyzer(model_name)\n",
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "analyzer.start_profiler()\n",
    "analyzer.inference_one(input_tensor)\n",
    "analyzer.stop_profiler()\n",
    "analyzer.list_events()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
