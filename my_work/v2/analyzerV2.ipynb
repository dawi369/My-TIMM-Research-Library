{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Optional\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "\n",
    "# Pytorch/TIMM libraries\n",
    "import timm\n",
    "from timm.models.helpers import model_parameters\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchprofile import profile_macs\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Data visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ace_tools_open as tools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Environment variables\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "# Commands\n",
    "# tensorboard --logdir=./my_work/v2/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [model for model in timm.list_models('vit*')]\n",
    "# print([x for x in models if 'vitamin' not in str(x)])\n",
    "model_name = \"vit_base_patch16_224\"\n",
    "\n",
    "#TODO Learn how to use SummaryWriter\n",
    "#TODO Learn what CUDA AMP is\n",
    "#TODO Event handler ignores [memory] event, probably useful \n",
    "#TODO Terrible looking plots in tensorboard, fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    def __init__(self, model_name, device=\"cpu\", pretrained=True, pretrained_cfg=None, pretrained_cfg_overlay=None,\n",
    "                checkpoint_path='', scriptable=None, exportable=None, no_jit=True):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(device if torch.cuda.is_available() and device == \"cuda\" else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, pretrained_cfg=pretrained_cfg, pretrained_cfg_overlay=pretrained_cfg_overlay,\n",
    "            checkpoint_path=checkpoint_path, scriptable=scriptable, exportable=exportable, no_jit=no_jit\n",
    "        ).to(self.device)\n",
    "        self.profiler = None  \n",
    "        \n",
    "        # print(self.model.default_cfg)\n",
    "        # params = sum(p.numel() for p in self.model.parameters())\n",
    "        # print(f\"Number of parameters: {params / 1e6:.2f}M\")\n",
    "    \n",
    "    def inference_one(self, input_tensor: torch.Tensor) -> None:\n",
    "        \"\"\"Run inference on a single input tensor.\"\"\"\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        self.model.eval()  \n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "            print(f\"Model device: {next(self.model.parameters()).device}\")\n",
    "            print(f\"Input tensor device: {input_tensor.device}\")\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def start_profiler(self, create_logfile=False) -> None:\n",
    "        \"\"\"Initialize and start the profiler.\"\"\"\n",
    "        if create_logfile:\n",
    "            trace_handler = torch.profiler.tensorboard_trace_handler('./logs')\n",
    "        else:\n",
    "            trace_handler = None\n",
    "            \n",
    "        self.profiler = profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            on_trace_ready=trace_handler,\n",
    "            record_shapes=True,\n",
    "            with_stack=True,\n",
    "            with_flops=True,\n",
    "            profile_memory=True,\n",
    "            with_modules=True\n",
    "        )\n",
    "        self.profiler.__enter__()  # Start the profiler context manually\n",
    "\n",
    "    def stop_profiler(self) -> None:\n",
    "        \"\"\"Stop the profiler and process the collected data.\"\"\"\n",
    "        if self.profiler:\n",
    "            self.profiler.__exit__(None, None, None) \n",
    "            \n",
    "    def is_cuda_initialized(self, logs=False) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.device.type != \"cuda\":\n",
    "                if logs:\t\n",
    "                    print(f\"Parameter {name} is not on GPU\")\n",
    "                else:\n",
    "                    print(\"Some or all model parameters are not on GPU\")\n",
    "                    return\n",
    "                # return False\n",
    "        print(\"All model parameters are on the GPU\")\n",
    "        # return True\n",
    "\n",
    "    def list_events(self, show_key_averages=False, list_all=False) -> None:\n",
    "        \"\"\"\n",
    "        List all recorded events from the profiler.\t\n",
    "        name\t         - The name of the operation (e.g., aten::add, aten::matmul, aten::conv2d).\n",
    "\t\tcpu_time_total\t - Total time spent on the CPU for this operation, in microseconds.\n",
    "\t\tcuda_time_total\t - Total time spent on the GPU for this operation, in microseconds.\n",
    "\t\tinput_shapes\t - Shapes of the tensors used as inputs to this operation.\n",
    "\t\toutput_shapes\t - Shapes of the tensors produced by this operation (if applicable).\n",
    "\t\tdevice_type\t     - Whether the operation was executed on CPU or CUDA.\n",
    "\t\tdevice\t         - The device ID on which the operation was executed.\n",
    "\t\tself_cpu_time\t - Time spent on the CPU for this operation alone (excluding time for child operations).\n",
    "\t\tself_cuda_time   - Time spent on the GPU for this operation alone (excluding time for child operations).\n",
    "\t\t\"\"\"\n",
    "        if self.profiler is None:\n",
    "            print(\"Profiler has not been initialized or profiling session has ended.\")\n",
    "        else:\n",
    "            if show_key_averages:\t\n",
    "                print(self.profiler.key_averages().table())\n",
    "            if list_all:\n",
    "                for event in self.profiler.events():\n",
    "                    print(f\"Name: {event.name}, CPU Time: {event.cpu_time_total}, CUDA Time: {event.cuda_time_total}\")\n",
    "    \n",
    "    def event_handler(self, create_csv_file=False, log_to_tensorboard=False, plot_events=False, log_dir='logs') -> None:\n",
    "        if self.profiler is None:\n",
    "            print(\"Profiler has not been initialized or profiling session has ended.\")\n",
    "        else:         \n",
    "            key_averages = self.profiler.key_averages()\n",
    "            \n",
    "            def helper_list_events(key_averages=key_averages):\n",
    "                # Helper code to print all events and their attributes\n",
    "                for i, event in enumerate(key_averages):\n",
    "                    print(f\"Event {i + 1} name: {event.key}\")\n",
    "                    attributes = [attr for attr in dir(event) if not attr.startswith(\"_\") and not callable(getattr(event, attr))]\n",
    "                    for attr in attributes:\n",
    "                        print(f\"  {attr}: {getattr(event, attr)}\")\n",
    "                    print(\"-\" * 50)  # Separator between events\n",
    "            # helper_list_events()\n",
    "                \n",
    "            profiler_data = []\n",
    "            \n",
    "            if log_to_tensorboard:\n",
    "                writer = SummaryWriter(log_dir=log_dir) \n",
    "                \n",
    "            # Usually collapsed \n",
    "            for event in key_averages:\n",
    "                if not event.key == \"[memory]\":\n",
    "                    cpu_children_time = event.cpu_time_total - event.self_cpu_time_total\n",
    "                    cuda_children_time = event.device_time_total - event.self_device_time_total\n",
    "                    \n",
    "                    event_data = {\n",
    "                        \"Name\": event.key,\n",
    "                        \"Count\": event.count,\n",
    "                        # Timing attributes\n",
    "                        \"CPU time op only\": event.self_cpu_time_total,\n",
    "                        \"CPU time total (+children)\": event.cpu_time_total,\n",
    "                        \"CPU children time\": cpu_children_time,\n",
    "\t\t\t\t\t\t\"CUDA time op only\": event.self_device_time_total,\n",
    "\t\t\t\t\t\t\"CUDA time total (+children)\": event.device_time_total,\n",
    "\t\t\t\t\t\t\"CUDA children time\": cuda_children_time,\t\n",
    "\t\t\t\t\t\t# Memory attributes\n",
    "\t\t\t\t\t\t\"CPU memory usage (+children)\": event.cpu_memory_usage,\n",
    "\t\t\t\t\t\t\"CPU memory usage op only\": event.self_cpu_memory_usage,\n",
    "\t\t\t\t\t\t\"CUDA memory usage (+children)\": event.device_memory_usage,\n",
    "\t\t\t\t\t\t\"CUDA memory usage op only\": event.self_device_memory_usage,\n",
    "\t\t\t\t\t\t# Performance\n",
    "\t\t\t\t\t\t\"Flops\": event.flops,\t\n",
    "\t\t\t\t\t\t\"Is Async\": event.is_async,\n",
    "\t\t\t\t\t\t\"Input Shapes\": event.input_shapes,\n",
    "\t\t\t\t\t\t\"Stack\": event.stack\n",
    "\t\t\t\t\t}\n",
    "                    \n",
    "                    profiler_data.append(event_data)\n",
    "                    \n",
    "                    # Log metrics to TensorBoard\n",
    "                    if log_to_tensorboard:\n",
    "                        writer.add_scalar(\"Performance/CPU_time_op_only\", event.self_cpu_time_total, global_step=event.count)\n",
    "                        writer.add_scalar(\"Performance/CPU_time_total\", event.cpu_time_total, global_step=event.count)\n",
    "                        writer.add_scalar(\"Performance/CPU_children_time\", cpu_children_time, global_step=event.count)\n",
    "                        writer.add_scalar(\"Performance/CUDA_time_op_only\", event.self_device_time_total, global_step=event.count)\n",
    "                        writer.add_scalar(\"Performance/CUDA_time_total\", event.device_time_total, global_step=event.count)\n",
    "                        writer.add_scalar(\"Performance/CUDA_children_time\", cuda_children_time, global_step=event.count)\n",
    "                        writer.add_scalar(\"Performance/FLOPS\", event.flops, global_step=event.count)\n",
    "                        writer.add_scalar(\"Memory/CPU_memory_usage_children\", event.cpu_memory_usage, global_step=event.count)\n",
    "                        writer.add_scalar(\"Memory/CUDA_memory_usage_children\", event.device_memory_usage, global_step=event.count)\n",
    "                        \n",
    "            df = pd.DataFrame(profiler_data)\n",
    "\t\n",
    "            if create_csv_file:\n",
    "                df.to_csv(f\"./operations/{self.model_name}_profiler_data.csv\", index=False)\n",
    "                \n",
    "            if log_to_tensorboard:\n",
    "                writer.close()\n",
    "                \n",
    "            if plot_events:\n",
    "                numeric_columns = [\n",
    "\t\t\t\t\t\"CPU time op only\", \"CPU time total (+children)\", \"CPU children time\",\n",
    "\t\t\t\t\t\"CUDA time op only\", \"CUDA time total (+children)\", \"CUDA children time\",\n",
    "\t\t\t\t\t\"CPU memory usage (+children)\", \"CPU memory usage op only\",\n",
    "\t\t\t\t\t\"CUDA memory usage (+children)\", \"CUDA memory usage op only\", \"Flops\"\n",
    "\t\t\t\t]\n",
    "                df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "                df = df.dropna()\n",
    "\n",
    "\t\t\t\t# Usually collapsed \n",
    "                def helper_plot_events(df=df):\n",
    "                    # CPU vs CUDA Time (Operation Only)\n",
    "                    plt.figure(figsize=(14, 6))\n",
    "                    df_melted = df.melt(\n",
    "\t\t\t\t\t\tid_vars=[\"Name\"], \n",
    "\t\t\t\t\t\tvalue_vars=[\"CPU time op only\", \"CUDA time op only\"],\n",
    "\t\t\t\t\t\tvar_name=\"Type\", \n",
    "\t\t\t\t\t\tvalue_name=\"Time (us)\"\n",
    "\t\t\t\t\t)\n",
    "                    sns.barplot(data=df_melted, x=\"Name\", y=\"Time (us)\", hue=\"Type\")\n",
    "                    plt.xticks(rotation=90)\n",
    "                    plt.title(\"CPU vs CUDA Time (Operation Only)\")\n",
    "                    plt.show()\n",
    "\t\t\t\t\t\n",
    "                helper_plot_events()  \n",
    "                \n",
    "        \n",
    "    def analyze_layers(self, create_csv=False) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Uses a recursive function to extract layer information from the model.\"\"\"\n",
    "        layers_data = []\n",
    "    \n",
    "        def extract_layer_info(name, module, parent_name=\"\") -> None:\n",
    "\t\t\t# Define layer_info here and update it dynamically\t\n",
    "            layer_info = {\n",
    "\t\t\t\t\"Name\": f\"{parent_name}.{name}\" if parent_name else name,\n",
    "\t\t\t\t\"Type\": module.__class__.__name__,\n",
    "\t\t\t\t\"Input Shape\": None,\n",
    "\t\t\t\t\"Output Shape\": None,\n",
    "\t\t\t\t\"MACs\": None,\n",
    "\t\t\t\t\"Memory Usage (bytes)\": None,\n",
    "\t\t\t\t\"Execution Time (ms)\": None,\n",
    "\t\t\t}\n",
    "                        \n",
    "            layers_data.append(layer_info)\n",
    "            \n",
    "        def recurse_layers(module, parent_name=\"\") -> None:\n",
    "            for name, child in module.named_children():\n",
    "                extract_layer_info(name, child, parent_name)\n",
    "                recurse_layers(child, parent_name=f\"{parent_name}.{name}\" if parent_name else name)\n",
    "\n",
    "        recurse_layers(self.model)\n",
    "\n",
    "        df = pd.DataFrame(layers_data)\n",
    "        if create_csv:\n",
    "            df.to_csv(f\"./layers/{self.model_name}_layers.csv\", index=False)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    # def analyze_layers(self, input_shape=(1, 3, 224, 224), create_csv=False) -> Optional[pd.DataFrame]:\n",
    "    #     layers_data = []\n",
    "\n",
    "    #     def extract_macs(module, input_shape):\n",
    "    #         try:\n",
    "    #             dummy_input = torch.randn(*input_shape).to(self.device)\n",
    "    #             flops = FlopCountAnalysis(module, dummy_input).total()\n",
    "    #             return flops\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error calculating FLOPs for {module}: {e}\")\n",
    "    #             return None\n",
    "\n",
    "    #     def extract_memory(module):\n",
    "    #         try:\n",
    "    #             memory = sum(p.numel() * p.element_size() for p in module.parameters())\n",
    "    #             return memory\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error calculating memory for {module}: {e}\")\n",
    "    #             return None\n",
    "\n",
    "    #     def extract_layer_info(name, module, parent_name=\"\"):\n",
    "\t# \t\t# Define layer_info here and update it dynamically\t\n",
    "    #         layer_info = {\n",
    "\t# \t\t\t\"Name\": f\"{parent_name}.{name}\" if parent_name else name,\n",
    "\t# \t\t\t\"Type\": module.__class__.__name__,\n",
    "\t# \t\t\t\"Parameters\": sum(p.numel() for p in module.parameters()),\n",
    "\t# \t\t\t\"Trainable Params\": sum(p.numel() for p in module.parameters() if p.requires_grad),\n",
    "\t# \t\t\t\"Non-Trainable Params\": sum(p.numel() for p in module.parameters() if not p.requires_grad),\n",
    "\t# \t\t\t\"Input Shape\": None,\n",
    "\t# \t\t\t\"Output Shape\": None,\n",
    "\t# \t\t\t\"MACs\": None,\n",
    "\t# \t\t\t\"Memory Usage (bytes)\": None,\n",
    "\t# \t\t\t\"Execution Time (ms)\": None,\n",
    "\t# \t\t}\n",
    "\n",
    "\t# \t\t# Hooks for Input/Output Shape\n",
    "    #         def get_layer_shapes(module):\n",
    "    #             def hook(module, input, output):\n",
    "    #                 layer_info[\"Input Shape\"] = input[0].shape if isinstance(input, tuple) else input.shape\n",
    "    #                 layer_info[\"Output Shape\"] = output.shape\n",
    "    #             return module.register_forward_hook(hook)\n",
    "\n",
    "\t# \t\t# Timing Hook\n",
    "    #         def get_layer_timing(module):\n",
    "    #             def hook(module, input, output):\n",
    "    #                 start_time = time.time()\n",
    "    #                 module(input)\n",
    "    #                 end_time = time.time()\n",
    "    #                 layer_info[\"Execution Time\"] = (end_time - start_time) * 1000  # Convert to ms\n",
    "    #             return module.register_forward_hook(hook)\n",
    "\n",
    "\t# \t\t# Register hooks and run dummy forward pass\n",
    "    #         hook_shapes = get_layer_shapes(module)\n",
    "    #         hook_timing = get_layer_timing(module)\n",
    "\n",
    "    #         try:\n",
    "    #             dummy_input = torch.randn(*input_shape).to(self.device)\n",
    "    #             module(dummy_input)\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error during forward pass for layer {name}: {e}\")\n",
    "    #         finally:\n",
    "    #             hook_shapes.remove()\n",
    "    #             hook_timing.remove()\n",
    "\n",
    "\t# \t\t# Calculate MACs and Memory Usage\n",
    "    #         layer_info[\"MACs\"] = extract_macs(module, input_shape)\n",
    "    #         layer_info[\"Memory Usage (bytes)\"] = extract_memory(module)\n",
    "\n",
    "    #         layers_data.append(layer_info)\n",
    "\n",
    "    #     def recurse_layers(module, parent_name=\"\"):\n",
    "    #         for name, child in module.named_children():\n",
    "    #             extract_layer_info(name, child, parent_name)\n",
    "    #             recurse_layers(child, parent_name=f\"{parent_name}.{name}\" if parent_name else name)\n",
    "\n",
    "    #     recurse_layers(self.model)\n",
    "\n",
    "    #     df = pd.DataFrame(layers_data)\n",
    "    #     if create_csv:\n",
    "    #         df.to_csv(f\"./layers/{self.model_name}_layers.csv\", index=False)\n",
    "\n",
    "    #     return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_logs():\n",
    "    for file in os.listdir('./logs'):\n",
    "        os.remove(os.path.join('./logs', file))\n",
    "def del_operations():\n",
    "    for file in os.listdir('./operations'):\n",
    "        os.remove(os.path.join('./operaions', file))\n",
    "def del_layers():\n",
    "    for file in os.listdir('./layers'):\n",
    "        os.remove(os.path.join('./layers', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "All model parameters are on the GPU\n"
     ]
    }
   ],
   "source": [
    "analyzer = Analyzer(model_name, device=\"cuda\")\n",
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "# analyzer.start_profiler(create_logfile=False)\n",
    "# analyzer.inference_one(input_tensor)\n",
    "# analyzer.stop_profiler()\n",
    "analyzer.is_cuda_initialized(logs=False)\n",
    "# analyzer.list_events(show_key_averages=False, list_all=False)\n",
    "# analyzer.event_handler(create_csv_file=False, log_to_tensorboard=False, plot_events=True, log_dir='logs')\n",
    "\n",
    "layers_df = analyzer.analyze_layers(create_csv=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
